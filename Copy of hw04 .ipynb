{"cells":[{"cell_type":"markdown","metadata":{"id":"C_jdZ5vHJ4A9"},"source":["# Task description\n","- Classify the speakers of given features.\n","- Main goal: Learn how to use transformer.\n","- Baselines:\n","  - Easy: Run sample code and know how to use transformer.\n","  - Medium: Know how to adjust parameters of transformer.\n","  - Strong: Construct [conformer](https://arxiv.org/abs/2005.08100) which is a variety of transformer.\n","  - Boss: Implement [Self-Attention Pooling](https://arxiv.org/pdf/2008.01077v1.pdf) & [Additive Margin Softmax](https://arxiv.org/pdf/1801.05599.pdf) to further boost the performance.\n","\n","- Other links\n","  - Kaggle: [link](https://www.kaggle.com/t/ac77388c90204a4c8daebeddd40ff916)\n","  - Slide: [link](https://docs.google.com/presentation/d/1HLAj7UUIjZOycDe7DaVLSwJfXVd3bXPOyzSb6Zk3hYU/edit?usp=sharing)\n","  - Data: [link](https://drive.google.com/drive/folders/1vI1kuLB-q1VilIftiwnPOCAeOOFfBZge?usp=sharing)\n","\n","# Download dataset\n","- Data is [here](https://drive.google.com/drive/folders/1vI1kuLB-q1VilIftiwnPOCAeOOFfBZge?usp=sharing)"]},{"cell_type":"markdown","source":["调Transformer参数+ConformerBlock: 0.773 0.77525"],"metadata":{"id":"LhwI6zCgFbSh"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"LhLNWB-AK2Z5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1757762615887,"user_tz":-480,"elapsed":236703,"user":{"displayName":"张雨欣","userId":"18331624392117621074"}},"outputId":"6b50fc6a-a28d-4fd7-cea7-68b8f0b7934d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","total 5.9G\n","drwxr-xr-x 1 root  root 4.0K Sep 13 11:19 .\n","drwxr-xr-x 1 root  root 4.0K Sep 13 11:13 ..\n","drwxr-xr-x 4 root  root 4.0K Sep  9 13:46 .config\n","drwxr-xr-x 2 69057  450 4.4M Mar 10  2022 Dataset\n","-rw------- 1 root  root 5.9G Sep 13 11:19 Dataset.tar.gz\n","drwx------ 5 root  root 4.0K Sep 13 11:17 drive\n","drwxr-xr-x 1 root  root 4.0K Sep  9 13:46 sample_data\n"]}],"source":["'''!wget https://github.com/MachineLearningHW/ML_HW4_Dataset/releases/latest/download/Dataset.tar.gz.partaa\n","!wget https://github.com/MachineLearningHW/ML_HW4_Dataset/releases/latest/download/Dataset.tar.gz.partab\n","!wget https://github.com/MachineLearningHW/ML_HW4_Dataset/releases/latest/download/Dataset.tar.gz.partac\n","!wget https://github.com/MachineLearningHW/ML_HW4_Dataset/releases/latest/download/Dataset.tar.gz.partad\n","!wget https://drive.google.com/drive/folders/1vI1kuLB-q1VilIftiwnPOCAeOOFfBZge\n","!cat Dataset.tar.gz.part* > Dataset.tar.gz\n","\n","# unzip the file\n","tar zxvf Dataset.tar.gz\n","# 安装 gdown（一次即可）\n","# 1) 把整个文件夹拉到当前目录的临时子目录\n","!gdown --folder \"https://drive.google.com/drive/folders/1vI1kuLB-q1VilIftiwnPOCAeOOFfBZge\" -O ./_dl_tmp\n","\n","# 2) 找到其中的 Dataset.tar.gz 并复制到当前目录\n","!cp \"$(find ./_dl_tmp -type f -name 'Dataset.tar.gz' | head -n 1)\" ./Dataset.tar.gz\n","\n","# 3) 解压到当前目录（等价于你写的 tar zxvf Dataset.tar.gz，只是少打印点日志）\n","!tar -xzf ./Dataset.tar.gz -C .\n","\n","# 4)（可选）确认解压结果\n","!ls -lah | head -n 50'''\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# 把“副本”复制到当前目录并解压\n","!cp \"/content/drive/MyDrive/CopyDataset.tar.gz\" ./Dataset.tar.gz\n","!tar -xzf ./Dataset.tar.gz -C .\n","!ls -lah | head -n 30\n","\n"]},{"cell_type":"markdown","source":[],"metadata":{"id":"5Aurqdwio0M8"}},{"cell_type":"markdown","metadata":{"id":"ENWVAUDVJtVY"},"source":["## Fix Random Seed"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"E6burzCXIyuA","executionInfo":{"status":"ok","timestamp":1757762619886,"user_tz":-480,"elapsed":3996,"user":{"displayName":"张雨欣","userId":"18331624392117621074"}}},"outputs":[],"source":["import numpy as np\n","import torch\n","import random\n","#随机数跟作业三一样\n","def set_seed(seed):\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed)\n","        torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.benchmark = False\n","    torch.backends.cudnn.deterministic = True\n","\n","set_seed(87)"]},{"cell_type":"markdown","metadata":{"id":"k7dVbxW2LASN"},"source":["# Data\n","\n","## Dataset\n","- Original dataset is [Voxceleb2](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox2.html).\n","- The [license](https://creativecommons.org/licenses/by/4.0/) and [complete version](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/files/license.txt) of Voxceleb2.\n","- We randomly select 600 speakers from Voxceleb2.\n","- Then preprocess the raw waveforms into mel-spectrograms.\n","\n","- Args:\n","  - data_dir: The path to the data directory.\n","  - metadata_path: The path to the metadata.\n","  - segment_len: The length of audio segment for training.\n","- The architecture of data directory \\\\\n","  - data directory \\\\\n","  |---- metadata.json \\\\\n","  |---- testdata.json \\\\\n","  |---- mapping.json \\\\\n","  |---- uttr-{random string}.pt \\\\\n","\n","- The information in metadata\n","  - \"n_mels\": The dimention of mel-spectrogram.\n","  - \"speakers\": A dictionary.\n","    - Key: speaker ids.\n","    - value: \"feature_path\" and \"mel_len\"\n","\n","\n","For efficiency, we segment the mel-spectrograms into segments in the traing step."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"KpuGxl4CI2pr","executionInfo":{"status":"ok","timestamp":1757762619888,"user_tz":-480,"elapsed":6,"user":{"displayName":"张雨欣","userId":"18331624392117621074"}}},"outputs":[],"source":["import os\n","import json\n","import torch\n","import random\n","from pathlib import Path\n","from torch.utils.data import Dataset\n","from torch.nn.utils.rnn import pad_sequence\n","\n","#处理数据集\n","class myDataset(Dataset):\n","\n","\tdef __init__(self, data_dir, segment_len=128):#每次取128帧的时间片\n","\t\tself.data_dir = data_dir\n","\t\tself.segment_len = segment_len#一帧时长=特征时长/采样率\n","\n","\t\t# Load the mapping from speaker neme to their corresponding id.获得说话人的id\n","\t\tmapping_path = Path(data_dir) / \"mapping.json\" #拼接成完整的路径\n","\t\tmapping = json.load(mapping_path.open())#打开获得json内容\n","\t\tself.speaker2id = mapping[\"speaker2id\"]#从中提取spk\n","\n","\t\t# Load metadata of training data.#获得spk的特征\n","\t\tmetadata_path = Path(data_dir) / \"metadata.json\"\n","\t\tmetadata = json.load(open(metadata_path))[\"speakers\"]#将json内容解析为python对象并取出speakers对应对的数据\n","\n","\t\t# Get the total number of speaker.\n","\t\tself.speaker_num = len(metadata.keys())#获得数量 .keys是返回字典的所有键\n","\t\tself.data = []\n","\t\tfor speaker in metadata.keys():\n","\t\t\tfor utterances in metadata[speaker]:#遍历说话人的所有语音条目\n","\t\t\t\tself.data.append([utterances[\"feature_path\"], self.speaker2id[speaker]])#构建【语音特征文件，id】的格式\n","\n","\tdef __len__(self):\n","\t\t\treturn len(self.data)\n","\n","\tdef __getitem__(self, index):\n","\t\tfeat_path, speaker = self.data[index]\n","\t\t# Load preprocessed mel-spectrogram.\n","\t\tmel = torch.load(os.path.join(self.data_dir, feat_path))#将.pt读成tensor 处理语音特征\n","\n","\t\t# Segmemt mel-spectrogram into \"segment_len\" frames.\n","\t\tif len(mel) > self.segment_len:#segment_len=128\n","\t\t\t# Randomly get the starting point of the segment.\n","\t\t\tstart = random.randint(0, len(mel) - self.segment_len)#随机找一个起点\n","\t\t\t# Get a segment with \"segment_len\" frames.\n","\t\t\tmel = torch.FloatTensor(mel[start:start+self.segment_len])#裁剪出128帧\n","\t\telse:\n","\t\t\tmel = torch.FloatTensor(mel)\n","\t\t# Turn the speaker id into long for computing loss later.\n","\t\tspeaker = torch.FloatTensor([speaker]).long()#返回原长度\n","\t\treturn mel, speaker\n","\n","\tdef get_speaker_number(self):\n","\t\treturn self.speaker_num"]},{"cell_type":"markdown","metadata":{"id":"668hverTMlGN"},"source":["## Dataloader\n","- Split dataset into training dataset(90%) and validation dataset(10%).\n","- Create dataloader to iterate the data."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"B7c2gZYoJDRS","executionInfo":{"status":"ok","timestamp":1757762619898,"user_tz":-480,"elapsed":2,"user":{"displayName":"张雨欣","userId":"18331624392117621074"}}},"outputs":[],"source":["import torch\n","from torch.utils.data import DataLoader, random_split\n","from torch.nn.utils.rnn import pad_sequence\n","\n","#将一个batch样本拼成张量\n","def collate_batch(batch):\n","\t# Process features within a batch.\n","\t\"\"\"Collate a batch of data.\"\"\"\n","\tmel, speaker = zip(*batch)#这个zip是按位置重新打包，输出是tuple类型\n","\t'''\n","\tbatch = [\n","    (mel1, 0),\n","    (mel2, 1),\n","    (mel3, 2),\n","]\n","zip((mel1, 0), (mel2, 1), (mel3, 2)) 再把mel、speaker分别放到一起\n","\t'''\n","\t# Because we train the model batch by batch, we need to pad the features in the same batch to make their lengths the same.\n","\t#补齐128 用-20补 因为-20是静音帧\n","\tmel = pad_sequence(mel, batch_first=True, padding_value=-20)    # pad log 10^(-20) which is very small value.\n","\t# mel: (batch size, length, 40)\n","\treturn mel, torch.FloatTensor(speaker).long()#将tuple转换为pytorch再变long类型的\n","\n","\n","def get_dataloader(data_dir, batch_size, n_workers):\n","\t\"\"\"Generate dataloader\"\"\"\n","\tdataset = myDataset(data_dir)\n","\tspeaker_num = dataset.get_speaker_number()\n","\t# Split dataset into training dataset and validation dataset\n","\t#拆分训练集和验证集\n","\ttrainlen = int(0.9 * len(dataset))\n","\tlengths = [trainlen, len(dataset) - trainlen]\n","\ttrainset, validset = random_split(dataset, lengths)\n","\n","\ttrain_loader = DataLoader(\n","\t\ttrainset,\n","\t\tbatch_size=batch_size,\n","\t\tshuffle=True,\n","\t\tdrop_last=True,\n","\t\tnum_workers=n_workers,\n","\t\tpin_memory=True,\n","\t\tcollate_fn=collate_batch,\n","\t)\n","\tvalid_loader = DataLoader(\n","\t\tvalidset,\n","\t\tbatch_size=batch_size,\n","\t\tnum_workers=n_workers,\n","\t\tdrop_last=True,\n","\t\tpin_memory=True,\n","\t\tcollate_fn=collate_batch,\n","\t)\n","\n","\treturn train_loader, valid_loader, speaker_num"]},{"cell_type":"markdown","metadata":{"id":"5FOSZYxrMqhc"},"source":["# Model\n","- TransformerEncoderLayer:\n","  - Base transformer encoder layer in [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n","  - Parameters:\n","    - d_model: the number of expected features of the input (required).\n","\n","    - nhead: the number of heads of the multiheadattention models (required).\n","\n","    - dim_feedforward: the dimension of the feedforward network model (default=2048).\n","\n","    - dropout: the dropout value (default=0.1).\n","\n","    - activation: the activation function of intermediate layer, relu or gelu (default=relu).\n","\n","- TransformerEncoder:\n","  - TransformerEncoder is a stack of N transformer encoder layers\n","  - Parameters:\n","    - encoder_layer: an instance of the TransformerEncoderLayer() class (required).\n","\n","    - num_layers: the number of sub-encoder-layers in the encoder (required).\n","\n","    - norm: the layer normalization component (optional)."]},{"cell_type":"code","source":["!pip -q install conformer\n"],"metadata":{"id":"PHVhTgQiYqev","executionInfo":{"status":"ok","timestamp":1757762624526,"user_tz":-480,"elapsed":4626,"user":{"displayName":"张雨欣","userId":"18331624392117621074"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from conformer import ConformerBlock\n","import copy\n","\n","class AMSoftmaxHead(nn.Module):\n","    \"\"\"\n","    Additive Margin Softmax head (a.k.a. CosFace-style):\n","    - L2 normalize feature x and weight W\n","    - compute cosine logits: cos(theta) = x̂ · Ŵ\n","    - output scaled cosine: s * cos(theta)\n","    \"\"\"\n","    def __init__(self, in_dim: int, n_classes: int, s: float = 30.0):\n","        super().__init__()\n","        self.W = nn.Parameter(torch.empty(n_classes, in_dim))\n","        nn.init.xavier_uniform_(self.W)\n","        self.s = s  # scaling factor\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        # x: [B, D]; W: [C, D]\n","        x_norm = F.normalize(x, dim=-1)              # [B, D]\n","        W_norm = F.normalize(self.W, dim=-1)         # [C, D]\n","        cos = torch.matmul(x_norm, W_norm.t())       # [B, C]\n","        return self.s * cos                           # scaled cosine logits\n","\n","\n","class AMSoftmaxLoss(nn.Module):\n","    \"\"\"\n","    Cross-Entropy on (s * (cos - m) for target class, cos for others)\n","    \"\"\"\n","    def __init__(self, m: float = 0.20, s: float = 30.0):\n","        super().__init__()\n","        self.m = m\n","        self.s = s\n","\n","    def forward(self, cosine_scaled: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n","        # cosine_scaled = s * cos(theta)\n","        # We subtract margin in the target logit BEFORE softmax.\n","        logits = cosine_scaled.clone()\n","        # subtract s*m (because logits already have s)\n","        logits[torch.arange(logits.size(0)), target] -= self.s * self.m\n","        return F.cross_entropy(logits, target)\n","\n","class SAP(nn.Module):\n","    def __init__(self, in_dim, hidden=128):\n","        super().__init__()\n","        self.attn = nn.Sequential(\n","            nn.Linear(in_dim, hidden), nn.Tanh(),\n","            nn.Linear(hidden, 1, bias=False),\n","        )\n","    def forward(self, x, mask=None):          # x: [B,T,D]\n","        e = self.attn(x).squeeze(-1)          # [B,T]\n","        if mask is not None:\n","            e = e.masked_fill(~mask, -1e9)\n","        w = e.softmax(dim=1)                  # [B,T]\n","        return torch.bmm(w.unsqueeze(1), x).squeeze(1)  # [B,D]\n","\n","class Classifier(nn.Module):\n","    # d_model: 编码维度；n_spks: 说话人数；n_layers: Conformer层数；n_mels: 梅尔维\n","    def __init__(self, d_model=224, n_spks=600, dropout=0.2):\n","        super().__init__()\n","        self.prenet = nn.Linear(40, d_model)\n","\t#输入投影层\n","\t\t# Project the dimension of features from that of input into d_model.\n","\t\t#self.prenet = nn.Linear(40, d_model)#将mel维度升为80\n","\t\t# TODO:\n","\t\t#   Change Transformer to Conformer.\n","\t\t#   https://arxiv.org/abs/2005.08100\n","\t\t#Transformer编码层\n","\t\t#self.encoder_layer = nn.TransformerEncoderLayer(\n","\t\t\t#d_model=d_model, dim_feedforward=d_model*2, nhead=2,dropout=dropout)\n","\t\t#self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=3)\n","        block = ConformerBlock(\n","            dim=d_model,\n","            dim_head=4, heads=4, ff_mult=4,\n","            conv_expansion_factor=2, conv_kernel_size=20,\n","            attn_dropout=dropout, ff_dropout=dropout, conv_dropout=dropout,\n","        )\n","        self.encoder = nn.Sequential(*[copy.deepcopy(block) for _ in range(3)])\n","\n","        self.pool = SAP(d_model)\n","        '''\n","        self.pred_layer = nn.Sequential(\n","            nn.BatchNorm1d(d_model),\n","            nn.Linear(d_model, n_spks),\n","        )'''\n","        self.pred_layer = AMSoftmaxHead(in_dim=d_model, n_classes=n_spks, s=30.0)\n","    def forward(self, mels):\n","      # out: (batch size, length, d_model)\n","     out=self.prenet(mels)\n","     # out: (length, batch size, d_model)PyTorch 的 nn.TransformerEncoderLayer 要求输入维度是 (time, batch, feature)。\n","\t\t#out = out.permute(1, 0, 2)\n","\t\t# The encoder layer expect features in the shape of (length, batch size, d_model).\n","\t\t#SAP是batchfirst 不用重排了\n","     out = self.encoder(out)\n","     # out: (batch size, length, d_model)\n","\t\t#out = out.transpose(0, 1)\n","\t\t# mean pooling\n","\t\t#stats = out.mean(dim=1)\n","\t\t#用SAP\n","     stats = self.pool(out)\n","     # out: (batch, n_spks) 分类层\n","     out = self.pred_layer(stats)\n","     return out\n"],"metadata":{"id":"X_3VbAAwa6it","executionInfo":{"status":"ok","timestamp":1757762632260,"user_tz":-480,"elapsed":7728,"user":{"displayName":"张雨欣","userId":"18331624392117621074"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W7yX8JinM5Ly"},"source":["# Learning rate schedule\n","- For transformer architecture, the design of learning rate schedule is different from that of CNN.\n","- Previous works show that the warmup of learning rate is useful for training models with transformer architectures.\n","- The warmup schedule\n","  - Set learning rate to 0 in the beginning.\n","  - The learning rate increases linearly from 0 to initial learning rate during warmup period."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"ykt0N1nVJJi2","executionInfo":{"status":"ok","timestamp":1757762632284,"user_tz":-480,"elapsed":15,"user":{"displayName":"张雨欣","userId":"18331624392117621074"}}},"outputs":[],"source":["#Warmup+余弦退火\n","\n","import math\n","\n","import torch\n","from torch.optim import Optimizer\n","from torch.optim.lr_scheduler import LambdaLR\n","\n","\n","def get_cosine_schedule_with_warmup(\n","\toptimizer: Optimizer,\n","\tnum_warmup_steps: int,\n","\tnum_training_steps: int,\n","\tnum_cycles: float = 0.5,\n","\tlast_epoch: int = -1,\n","):\n","\t\"\"\"\n","\tCreate a schedule with a learning rate that decreases following the values of the cosine function between the\n","\tinitial lr set in the optimizer to 0, after a warmup period during which it increases linearly between 0 and the\n","\tinitial lr set in the optimizer.\n","\n","\tArgs:\n","\t\toptimizer (:class:`~torch.optim.Optimizer`):\n","\t\tThe optimizer for which to schedule the learning rate.\n","\t\tnum_warmup_steps (:obj:`int`):\n","\t\tThe number of steps for the warmup phase.\n","\t\tnum_training_steps (:obj:`int`):\n","\t\tThe total number of training steps.\n","\t\tnum_cycles (:obj:`float`, `optional`, defaults to 0.5):\n","\t\tThe number of waves in the cosine schedule (the defaults is to just decrease from the max value to 0\n","\t\tfollowing a half-cosine).\n","\t\tlast_epoch (:obj:`int`, `optional`, defaults to -1):\n","\t\tThe index of the last epoch when resuming training.\n","\n","\tReturn:\n","\t\t:obj:`torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n","\t\"\"\"\n","\tdef lr_lambda(current_step):\n","\t\t# Warmup\n","\t\tif current_step < num_warmup_steps:\n","\t\t\treturn float(current_step) / float(max(1, num_warmup_steps))\n","\t\t# decadence余弦退火\n","\t\tprogress = float(current_step - num_warmup_steps) / float(\n","\t\t\tmax(1, num_training_steps - num_warmup_steps)\n","\t\t)\n","\t\treturn max(\n","\t\t\t0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))\n","\t\t)\n","\n","\treturn LambdaLR(optimizer, lr_lambda, last_epoch)"]},{"cell_type":"markdown","metadata":{"id":"-LN2XkteM_uH"},"source":["# Model Function\n","- Model forward function."]},{"cell_type":"code","execution_count":8,"metadata":{"id":"N-rr8529JMz0","executionInfo":{"status":"ok","timestamp":1757762632380,"user_tz":-480,"elapsed":95,"user":{"displayName":"张雨欣","userId":"18331624392117621074"}}},"outputs":[],"source":["import torch\n","\n","\n","def model_fn(batch, model, criterion, device):\n","\t\"\"\"Forward a batch through the model.\"\"\"\n","\n","\tmels, labels = batch\n","\n","\tmels = mels.to(device)\n","\tlabels = labels.to(device)\n","\n","\touts = model(mels)#model继承的nn.Module的\n","\n","\tloss = criterion(outs, labels)\n","\n","\t# Get the speaker id with highest probability.\n","\tpreds = outs.argmax(1)\n","\t# Compute accuracy.\n","\taccuracy = torch.mean((preds == labels).float())\n","\n","\treturn loss, accuracy"]},{"cell_type":"markdown","metadata":{"id":"cwM_xyOtNCI2"},"source":["# Validate\n","- Calculate accuracy of the validation set."]},{"cell_type":"code","execution_count":9,"metadata":{"id":"YAiv6kpdJRTJ","executionInfo":{"status":"ok","timestamp":1757762632381,"user_tz":-480,"elapsed":10,"user":{"displayName":"张雨欣","userId":"18331624392117621074"}}},"outputs":[],"source":["from tqdm import tqdm\n","import torch\n","\n","\n","def valid(dataloader, model, criterion, device):\n","\t\"\"\"Validate on validation set.\"\"\"\n","\n","\tmodel.eval()\n","\trunning_loss = 0.0\n","\trunning_accuracy = 0.0\n","\tpbar = tqdm(total=len(dataloader.dataset), ncols=0, desc=\"Valid\", unit=\" uttr\")\n","\n","\tfor i, batch in enumerate(dataloader):\n","\t\twith torch.no_grad():\n","\t\t\tloss, accuracy = model_fn(batch, model, criterion, device)\n","\t\t\trunning_loss += loss.item()\n","\t\t\trunning_accuracy += accuracy.item()\n","\n","\t\tpbar.update(dataloader.batch_size)\n","\t\tpbar.set_postfix(\n","\t\t\tloss=f\"{running_loss / (i+1):.2f}\",\n","\t\t\taccuracy=f\"{running_accuracy / (i+1):.2f}\",\n","\t\t)\n","\n","\tpbar.close()\n","\tmodel.train()\n","\n","\treturn running_accuracy / len(dataloader)"]},{"cell_type":"markdown","metadata":{"id":"g6ne9G-eNEdG"},"source":["# Main function"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"Usv9s-CuJSG7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1757766767506,"user_tz":-480,"elapsed":4135133,"user":{"displayName":"张雨欣","userId":"18331624392117621074"}},"outputId":"571b56e5-dfef-4a41-ce53-a83ce8685e9a"},"outputs":[{"output_type":"stream","name":"stdout","text":["[Info]: Use cuda now!\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["[Info]: Finish loading data!\n","[Info]: Finish creating model!\n"]},{"output_type":"stream","name":"stderr","text":["Train: 100% 2000/2000 [02:00<00:00, 16.58 step/s, accuracy=0.34, loss=2.90, step=2000]\n","Valid: 100% 5664/5667 [00:06<00:00, 822.70 uttr/s, accuracy=0.26, loss=3.47] \n","Train: 100% 2000/2000 [01:54<00:00, 17.52 step/s, accuracy=0.41, loss=1.99, step=4000]\n","Valid: 100% 5664/5667 [00:06<00:00, 904.91 uttr/s, accuracy=0.49, loss=2.24] \n","Train: 100% 2000/2000 [01:56<00:00, 17.20 step/s, accuracy=0.47, loss=1.78, step=6000]\n","Valid: 100% 5664/5667 [00:06<00:00, 851.16 uttr/s, accuracy=0.61, loss=1.65] \n","Train: 100% 2000/2000 [01:54<00:00, 17.45 step/s, accuracy=0.78, loss=0.89, step=8000]\n","Valid: 100% 5664/5667 [00:04<00:00, 1291.76 uttr/s, accuracy=0.68, loss=1.34]\n","Train: 100% 2000/2000 [01:52<00:00, 17.80 step/s, accuracy=0.88, loss=0.59, step=1e+4]\n","Valid: 100% 5664/5667 [00:05<00:00, 1079.03 uttr/s, accuracy=0.74, loss=1.11]\n","Train:   0% 3/2000 [00:00<02:25, 13.68 step/s, accuracy=0.75, loss=0.94, step=1e+4]"]},{"output_type":"stream","name":"stdout","text":["Step 10000, best model saved. (accuracy=0.7373)\n"]},{"output_type":"stream","name":"stderr","text":["Train: 100% 2000/2000 [01:53<00:00, 17.67 step/s, accuracy=0.88, loss=0.56, step=12000]\n","Valid: 100% 5664/5667 [00:05<00:00, 1072.38 uttr/s, accuracy=0.76, loss=0.99]\n","Train: 100% 2000/2000 [01:52<00:00, 17.72 step/s, accuracy=0.75, loss=0.74, step=14000]\n","Valid: 100% 5664/5667 [00:05<00:00, 1027.44 uttr/s, accuracy=0.78, loss=0.91]\n","Train: 100% 2000/2000 [01:53<00:00, 17.65 step/s, accuracy=0.91, loss=0.30, step=16000]\n","Valid: 100% 5664/5667 [00:04<00:00, 1299.84 uttr/s, accuracy=0.82, loss=0.76]\n","Train: 100% 2000/2000 [01:52<00:00, 17.84 step/s, accuracy=0.78, loss=0.58, step=18000]\n","Valid: 100% 5664/5667 [00:04<00:00, 1317.11 uttr/s, accuracy=0.83, loss=0.72]\n","Train: 100% 2000/2000 [01:52<00:00, 17.72 step/s, accuracy=0.94, loss=0.30, step=2e+4]\n","Valid: 100% 5664/5667 [00:04<00:00, 1324.37 uttr/s, accuracy=0.84, loss=0.69]\n","Train:   0% 3/2000 [00:00<02:10, 15.31 step/s, accuracy=0.91, loss=0.34, step=2e+4]"]},{"output_type":"stream","name":"stdout","text":["Step 20000, best model saved. (accuracy=0.8351)\n"]},{"output_type":"stream","name":"stderr","text":["Train: 100% 2000/2000 [01:52<00:00, 17.75 step/s, accuracy=0.84, loss=0.54, step=22000]\n","Valid: 100% 5664/5667 [00:05<00:00, 1034.65 uttr/s, accuracy=0.84, loss=0.66]\n","Train: 100% 2000/2000 [01:52<00:00, 17.75 step/s, accuracy=0.97, loss=0.19, step=24000]\n","Valid: 100% 5664/5667 [00:05<00:00, 1036.17 uttr/s, accuracy=0.87, loss=0.58]\n","Train: 100% 2000/2000 [01:51<00:00, 17.88 step/s, accuracy=0.97, loss=0.24, step=26000]\n","Valid: 100% 5664/5667 [00:04<00:00, 1303.49 uttr/s, accuracy=0.86, loss=0.57]\n","Train: 100% 2000/2000 [01:52<00:00, 17.76 step/s, accuracy=0.97, loss=0.23, step=28000]\n","Valid: 100% 5664/5667 [00:04<00:00, 1326.92 uttr/s, accuracy=0.87, loss=0.54]\n","Train: 100% 2000/2000 [01:51<00:00, 17.87 step/s, accuracy=0.91, loss=0.35, step=3e+4]\n","Valid: 100% 5664/5667 [00:05<00:00, 1112.67 uttr/s, accuracy=0.88, loss=0.52]\n","Train:   0% 3/2000 [00:00<02:28, 13.47 step/s, accuracy=0.84, loss=0.37, step=3e+4]"]},{"output_type":"stream","name":"stdout","text":["Step 30000, best model saved. (accuracy=0.8759)\n"]},{"output_type":"stream","name":"stderr","text":["Train: 100% 2000/2000 [01:53<00:00, 17.66 step/s, accuracy=0.88, loss=0.43, step=32000]\n","Valid: 100% 5664/5667 [00:04<00:00, 1171.21 uttr/s, accuracy=0.90, loss=0.47]\n","Train: 100% 2000/2000 [01:52<00:00, 17.79 step/s, accuracy=0.97, loss=0.21, step=34000]\n","Valid: 100% 5664/5667 [00:04<00:00, 1318.71 uttr/s, accuracy=0.89, loss=0.49]\n","Train: 100% 2000/2000 [01:52<00:00, 17.73 step/s, accuracy=0.97, loss=0.14, step=36000]\n","Valid: 100% 5664/5667 [00:04<00:00, 1295.65 uttr/s, accuracy=0.89, loss=0.46]\n","Train: 100% 2000/2000 [01:52<00:00, 17.77 step/s, accuracy=1.00, loss=0.05, step=38000]\n","Valid: 100% 5664/5667 [00:05<00:00, 945.22 uttr/s, accuracy=0.90, loss=0.44] \n","Train: 100% 2000/2000 [01:53<00:00, 17.56 step/s, accuracy=1.00, loss=0.04, step=4e+4]\n","Valid: 100% 5664/5667 [00:04<00:00, 1287.32 uttr/s, accuracy=0.91, loss=0.40]\n","Train:   0% 4/2000 [00:00<01:55, 17.31 step/s, accuracy=0.97, loss=0.10, step=4e+4]"]},{"output_type":"stream","name":"stdout","text":["Step 40000, best model saved. (accuracy=0.9064)\n"]},{"output_type":"stream","name":"stderr","text":["Train: 100% 2000/2000 [01:52<00:00, 17.75 step/s, accuracy=1.00, loss=0.03, step=42000]\n","Valid: 100% 5664/5667 [00:04<00:00, 1314.88 uttr/s, accuracy=0.91, loss=0.39]\n","Train: 100% 2000/2000 [01:52<00:00, 17.71 step/s, accuracy=1.00, loss=0.03, step=44000]\n","Valid: 100% 5664/5667 [00:04<00:00, 1296.63 uttr/s, accuracy=0.91, loss=0.40]\n","Train: 100% 2000/2000 [01:52<00:00, 17.71 step/s, accuracy=1.00, loss=0.04, step=46000]\n","Valid: 100% 5664/5667 [00:05<00:00, 1076.58 uttr/s, accuracy=0.91, loss=0.38]\n","Train: 100% 2000/2000 [01:53<00:00, 17.66 step/s, accuracy=0.97, loss=0.14, step=48000]\n","Valid: 100% 5664/5667 [00:04<00:00, 1137.88 uttr/s, accuracy=0.92, loss=0.34]\n","Train: 100% 2000/2000 [01:51<00:00, 17.87 step/s, accuracy=1.00, loss=0.06, step=5e+4]\n","Valid: 100% 5664/5667 [00:04<00:00, 1327.50 uttr/s, accuracy=0.92, loss=0.35]\n","Train:   0% 4/2000 [00:00<01:55, 17.32 step/s, accuracy=0.97, loss=0.11, step=5e+4]"]},{"output_type":"stream","name":"stdout","text":["Step 50000, best model saved. (accuracy=0.9227)\n"]},{"output_type":"stream","name":"stderr","text":["Train: 100% 2000/2000 [01:52<00:00, 17.80 step/s, accuracy=1.00, loss=0.06, step=52000]\n","Valid: 100% 5664/5667 [00:04<00:00, 1259.25 uttr/s, accuracy=0.93, loss=0.32]\n","Train: 100% 2000/2000 [01:53<00:00, 17.70 step/s, accuracy=0.97, loss=0.08, step=54000]\n","Valid: 100% 5664/5667 [00:05<00:00, 1083.19 uttr/s, accuracy=0.92, loss=0.35]\n","Train: 100% 2000/2000 [01:53<00:00, 17.56 step/s, accuracy=1.00, loss=0.04, step=56000]\n","Valid: 100% 5664/5667 [00:05<00:00, 1131.52 uttr/s, accuracy=0.93, loss=0.33]\n","Train: 100% 2000/2000 [01:52<00:00, 17.75 step/s, accuracy=1.00, loss=0.03, step=58000]\n","Valid: 100% 5664/5667 [00:04<00:00, 1292.20 uttr/s, accuracy=0.93, loss=0.32]\n","Train: 100% 2000/2000 [01:53<00:00, 17.67 step/s, accuracy=1.00, loss=0.04, step=6e+4]\n","Valid: 100% 5664/5667 [00:04<00:00, 1316.60 uttr/s, accuracy=0.93, loss=0.31]\n","Train:   0% 4/2000 [00:00<01:55, 17.31 step/s, accuracy=1.00, loss=0.04, step=6e+4]"]},{"output_type":"stream","name":"stdout","text":["Step 60000, best model saved. (accuracy=0.9322)\n"]},{"output_type":"stream","name":"stderr","text":["Train: 100% 2000/2000 [01:53<00:00, 17.63 step/s, accuracy=1.00, loss=0.02, step=62000]\n","Valid: 100% 5664/5667 [00:04<00:00, 1173.14 uttr/s, accuracy=0.93, loss=0.31]\n","Train: 100% 2000/2000 [01:52<00:00, 17.85 step/s, accuracy=1.00, loss=0.04, step=64000]\n","Valid: 100% 5664/5667 [00:06<00:00, 812.89 uttr/s, accuracy=0.93, loss=0.30] \n","Train: 100% 2000/2000 [01:51<00:00, 17.92 step/s, accuracy=1.00, loss=0.03, step=66000]\n","Valid: 100% 5664/5667 [00:04<00:00, 1241.39 uttr/s, accuracy=0.93, loss=0.31]\n","Train: 100% 2000/2000 [01:52<00:00, 17.75 step/s, accuracy=1.00, loss=0.04, step=68000]\n","Valid: 100% 5664/5667 [00:04<00:00, 1315.32 uttr/s, accuracy=0.93, loss=0.32]\n","Train: 100% 2000/2000 [01:52<00:00, 17.80 step/s, accuracy=1.00, loss=0.02, step=7e+4]\n","Valid: 100% 5664/5667 [00:04<00:00, 1270.22 uttr/s, accuracy=0.93, loss=0.30]\n","Train:   0% 0/2000 [00:00<?, ? step/s]\n"]},{"output_type":"stream","name":"stdout","text":["Step 70000, best model saved. (accuracy=0.9322)\n"]}],"source":["from tqdm import tqdm\n","\n","import torch\n","import torch.nn as nn\n","from torch.optim import AdamW\n","from torch.utils.data import DataLoader, random_split\n","\n","\n","def parse_args():\n","\t\"\"\"arguments\"\"\"\n","\tconfig = {\n","\t\t\"data_dir\": \"./Dataset\",\n","\t\t\"save_path\": \"model.ckpt\",\n","\t\t\"batch_size\": 32,\n","\t\t\"n_workers\": 8,\n","\t\t\"valid_steps\": 2000,\n","\t\t\"warmup_steps\": 1000,\n","\t\t\"save_steps\": 10000,\n","\t\t\"total_steps\": 70000,\n","\t}\n","\n","\treturn config\n","\n","\n","def main(\n","\tdata_dir,\n","\tsave_path,\n","\tbatch_size,\n","\tn_workers,\n","\tvalid_steps,\n","\twarmup_steps,\n","\ttotal_steps,\n","\tsave_steps,\n","):\n","\t\"\"\"Main function.\"\"\"\n","\tdevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\tprint(f\"[Info]: Use {device} now!\")\n","\n","\ttrain_loader, valid_loader, speaker_num = get_dataloader(data_dir, batch_size, n_workers)\n","\ttrain_iterator = iter(train_loader)\n","\tprint(f\"[Info]: Finish loading data!\",flush = True)\n","\n","\tmodel = Classifier(n_spks=speaker_num).to(device)\n","\tcriterion = nn.CrossEntropyLoss()\n","\toptimizer = AdamW(model.parameters(), lr=1e-3)\n","\tscheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n","\tprint(f\"[Info]: Finish creating model!\",flush = True)\n","\n","\n","\n","\tbest_accuracy = -1.0\n","\tbest_state_dict = None\n","\n","\tpbar = tqdm(total=valid_steps, ncols=0, desc=\"Train\", unit=\" step\")\n","\n","\tfor step in range(total_steps):\n","\t\t# Get data\n","\t\ttry:\n","\t\t\tbatch = next(train_iterator)\n","\t\texcept StopIteration:\n","\t\t\ttrain_iterator = iter(train_loader)\n","\t\t\tbatch = next(train_iterator)\n","\n","\t\tloss, accuracy = model_fn(batch, model, criterion, device)\n","\t\tbatch_loss = loss.item()\n","\t\tbatch_accuracy = accuracy.item()\n","\n","\t\t# Updata model\n","\t\tloss.backward()\n","\t\toptimizer.step()\n","\t\tscheduler.step()\n","\t\toptimizer.zero_grad()\n","\n","\t\t# Log\n","\t\tpbar.update()\n","\t\tpbar.set_postfix(\n","\t\t\tloss=f\"{batch_loss:.2f}\",\n","\t\t\taccuracy=f\"{batch_accuracy:.2f}\",\n","\t\t\tstep=step + 1,\n","\t\t)\n","\n","\t\t# Do validation\n","\t\tif (step + 1) % valid_steps == 0:\n","\t\t\tpbar.close()\n","\n","\t\t\tvalid_accuracy = valid(valid_loader, model, criterion, device)\n","\n","\t\t\t# keep the best model\n","\t\t\tif valid_accuracy > best_accuracy:\n","\t\t\t\tbest_accuracy = valid_accuracy\n","\t\t\t\tbest_state_dict = model.state_dict()\n","\n","\t\t\tpbar = tqdm(total=valid_steps, ncols=0, desc=\"Train\", unit=\" step\")\n","\n","\t\t# Save the best model so far.\n","\t\tif (step + 1) % save_steps == 0 and best_state_dict is not None:\n","\t\t\ttorch.save(best_state_dict, save_path)\n","\t\t\tpbar.write(f\"Step {step + 1}, best model saved. (accuracy={best_accuracy:.4f})\")\n","\n","\tpbar.close()\n","\n","\n","if __name__ == \"__main__\":\n","\tmain(**parse_args())"]},{"cell_type":"markdown","metadata":{"id":"NLatBYAhNNMx"},"source":["# Inference\n","\n","## Dataset of inference"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"efS4pCmAJXJH","executionInfo":{"status":"ok","timestamp":1757766767509,"user_tz":-480,"elapsed":2,"user":{"displayName":"张雨欣","userId":"18331624392117621074"}}},"outputs":[],"source":["import os\n","import json\n","import torch\n","from pathlib import Path\n","from torch.utils.data import Dataset\n","\n","\n","class InferenceDataset(Dataset):\n","\tdef __init__(self, data_dir):\n","\t\ttestdata_path = Path(data_dir) / \"testdata.json\"\n","\t\tmetadata = json.load(testdata_path.open())\n","\t\tself.data_dir = data_dir\n","\t\tself.data = metadata[\"utterances\"]\n","\n","\tdef __len__(self):\n","\t\treturn len(self.data)\n","\n","\tdef __getitem__(self, index):\n","\t\tutterance = self.data[index]\n","\t\tfeat_path = utterance[\"feature_path\"]\n","\t\tmel = torch.load(os.path.join(self.data_dir, feat_path))\n","\n","\t\treturn feat_path, mel\n","\n","\n","def inference_collate_batch(batch):\n","\t\"\"\"Collate a batch of data.\"\"\"\n","\tfeat_paths, mels = zip(*batch)\n","\n","\treturn feat_paths, torch.stack(mels)"]},{"cell_type":"markdown","metadata":{"id":"tl0WnYwxNK_S"},"source":["## Main funcrion of Inference"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"i8SAbuXEJb2A","colab":{"base_uri":"https://localhost:8080/","height":105,"referenced_widgets":["1480f5aa5a004499a80309fa89dd19b6","fdcd148c0793443e8bcc150ad4d3c659","eb8825eba8d542a8a88eac6709140717","bb186784849d4c9897abdfb8521f51ec","4bc55d09c86d494a85fc810eab48bbfa","4a1c7b3d2a0540beb286977747cbf9cd","90d79b4b1f2241678ffda5be59612a76","788f212ff0c6400183abf77472e3720a","08e8b71caaba427089aee275d9e429ae","8417005e8c82486aa0284ae404a30548","eb289b6100a446c0a873cf852a4537c9"]},"executionInfo":{"status":"ok","timestamp":1757766870774,"user_tz":-480,"elapsed":103249,"user":{"displayName":"张雨欣","userId":"18331624392117621074"}},"outputId":"d4871c0f-3864-4fa9-f711-201023931361"},"outputs":[{"output_type":"stream","name":"stdout","text":["[Info]: Use cuda now!\n","[Info]: Finish loading data!\n","[Info]: Finish creating model!\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/8000 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1480f5aa5a004499a80309fa89dd19b6"}},"metadata":{}}],"source":["import json\n","import csv\n","from pathlib import Path\n","from tqdm.notebook import tqdm\n","\n","import torch\n","from torch.utils.data import DataLoader\n","\n","def parse_args():\n","\t\"\"\"arguments\"\"\"\n","\tconfig = {\n","\t\t\"data_dir\": \"./Dataset\",\n","\t\t\"model_path\": \"./model.ckpt\",\n","\t\t\"output_path\": \"./output.csv\",\n","\t}\n","\n","\treturn config\n","\n","\n","def main(\n","\tdata_dir,\n","\tmodel_path,\n","\toutput_path,\n","):\n","\t\"\"\"Main function.\"\"\"\n","\tdevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\tprint(f\"[Info]: Use {device} now!\")\n","\n","\tmapping_path = Path(data_dir) / \"mapping.json\"\n","\tmapping = json.load(mapping_path.open())\n","\n","\tdataset = InferenceDataset(data_dir)\n","\tdataloader = DataLoader(\n","\t\tdataset,\n","\t\tbatch_size=1,\n","\t\tshuffle=False,\n","\t\tdrop_last=False,\n","\t\tnum_workers=8,\n","\t\tcollate_fn=inference_collate_batch,\n","\t)\n","\tprint(f\"[Info]: Finish loading data!\",flush = True)\n","\n","\tspeaker_num = len(mapping[\"id2speaker\"])\n","\tmodel = Classifier(n_spks=speaker_num).to(device)\n","\tmodel.load_state_dict(torch.load(model_path))\n","\tmodel.eval()\n","\tprint(f\"[Info]: Finish creating model!\",flush = True)\n","\n","\tresults = [[\"Id\", \"Category\"]]\n","\tfor feat_paths, mels in tqdm(dataloader):\n","\t\twith torch.no_grad():\n","\t\t\tmels = mels.to(device)\n","\t\t\touts = model(mels)\n","\t\t\tpreds = outs.argmax(1).cpu().numpy()\n","\t\t\tfor feat_path, pred in zip(feat_paths, preds):\n","\t\t\t\tresults.append([feat_path, mapping[\"id2speaker\"][str(pred)]])\n","\n","\twith open(output_path, 'w', newline='') as csvfile:\n","\t\twriter = csv.writer(csvfile)\n","\t\twriter.writerows(results)\n","\n","\n","if __name__ == \"__main__\":\n","\tmain(**parse_args())"]}],"metadata":{"colab":{"provenance":[{"file_id":"1wxm3lc2KYN328HXI2-FZyLhxrvf-uWbL","timestamp":1756467637533},{"file_id":"1gC2Gojv9ov9MUQ1a1WDpVBD6FOcLZsog","timestamp":1756371723325}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"1480f5aa5a004499a80309fa89dd19b6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fdcd148c0793443e8bcc150ad4d3c659","IPY_MODEL_eb8825eba8d542a8a88eac6709140717","IPY_MODEL_bb186784849d4c9897abdfb8521f51ec"],"layout":"IPY_MODEL_4bc55d09c86d494a85fc810eab48bbfa"}},"fdcd148c0793443e8bcc150ad4d3c659":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4a1c7b3d2a0540beb286977747cbf9cd","placeholder":"​","style":"IPY_MODEL_90d79b4b1f2241678ffda5be59612a76","value":"100%"}},"eb8825eba8d542a8a88eac6709140717":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_788f212ff0c6400183abf77472e3720a","max":8000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_08e8b71caaba427089aee275d9e429ae","value":8000}},"bb186784849d4c9897abdfb8521f51ec":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8417005e8c82486aa0284ae404a30548","placeholder":"​","style":"IPY_MODEL_eb289b6100a446c0a873cf852a4537c9","value":" 8000/8000 [01:42&lt;00:00, 70.54it/s]"}},"4bc55d09c86d494a85fc810eab48bbfa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4a1c7b3d2a0540beb286977747cbf9cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"90d79b4b1f2241678ffda5be59612a76":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"788f212ff0c6400183abf77472e3720a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"08e8b71caaba427089aee275d9e429ae":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8417005e8c82486aa0284ae404a30548":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb289b6100a446c0a873cf852a4537c9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}